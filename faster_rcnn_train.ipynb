{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sea Victims Detection - Faster RCNN Training\n\nhttps://www.kaggle.com/code/ubiratanfilho/sea-victims-detection-faster-rcnn","metadata":{"id":"DeVcuI9DXMD2"}},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:29.008990Z","iopub.execute_input":"2023-10-12T20:33:29.009311Z","iopub.status.idle":"2023-10-12T20:33:38.519707Z","shell.execute_reply.started":"2023-10-12T20:33:29.009283Z","shell.execute_reply":"2023-10-12T20:33:38.518612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport os\nimport argparse\nimport json\n\nimport pycocotools.coco as pyco\nfrom pycocotools.cocoeval import COCOeval\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport torch\nfrom torchvision.transforms import Compose, ToTensor, Resize\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchvision.models import resnet18, resnet50, resnet101,\\\n    ResNet101_Weights, ResNet18_Weights, ResNet50_Weights\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:38.521868Z","iopub.execute_input":"2023-10-12T20:33:38.522862Z","iopub.status.idle":"2023-10-12T20:33:41.789527Z","shell.execute_reply.started":"2023-10-12T20:33:38.522825Z","shell.execute_reply":"2023-10-12T20:33:41.788611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Class","metadata":{}},{"cell_type":"code","source":"class SDSDataset(Dataset):\n    def __init__(self, root, annotation_file, resize):\n        self.root = root\n        self.coco = pyco.COCO(annotation_file)\n        self.ids = list(self.coco.imgs.keys())\n        self.num_classes = len(self.coco.cats)\n        self.resize = resize\n        self.transform = Compose([\n            Resize(resize),\n            ToTensor()\n            ])\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        coco = self.coco\n\n        # Image ID of the input image\n        img_id = self.ids[index]\n        # Annotation IDs from coco\n        ann_ids = coco.getAnnIds(img_id)\n        # Load Annotation for the input image\n        coco_annotation = coco.loadAnns(ann_ids)\n        # Get path for the input image\n        path = coco.loadImgs(img_id)[0]['file_name']\n\n        # Open input image\n        org_image = Image.open(os.path.join(self.root, path))\n\n        # Get size of input image\n        org_height = org_image.height\n        org_width = org_image.width\n\n        # Apply transformation (resize) to input image\n        image = self.transform(org_image)\n\n        # Get number of objects in the input image\n        num_objects = len(coco_annotation)\n\n        # Get bounding boxes and category labels\n        # Coco format: bbox = [xmin, ymin, width, height]\n        # Pytorch format: bbox = [xmin, ymin, xmax, ymax]\n        boxes = []\n        labels = []\n        for i in range(num_objects):\n            # Convert and resize boxes\n            xmin = coco_annotation[i]['bbox'][0] / (org_width/self.resize[1])\n            ymin = coco_annotation[i]['bbox'][1] / (org_height/self.resize[0])\n            xmax = xmin + coco_annotation[i]['bbox'][2] / (org_width/self.resize[1])\n            ymax = ymin + coco_annotation[i]['bbox'][3] / (org_height/self.resize[0])\n            labels.append(coco_annotation[i]['category_id'])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        # Convert to tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        img_id = torch.tensor([img_id])\n\n        # Get (rectangular) size of bbox\n        areas = []\n        for i in range(num_objects):\n            areas.append(coco_annotation[i]['area'])\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n\n        # Get Iscrowd\n        iscrowd = torch.zeros((num_objects,), dtype=torch.int64)\n\n        # Create annotation dictionary\n        annotation = dict()\n        annotation['boxes'] = boxes\n        annotation['labels'] = labels\n        annotation['image_id'] = img_id\n        annotation['area'] = areas\n        annotation['iscrowd'] = iscrowd\n\n        # Save width and height of the original image to rescale bounding boxes later on\n        annotation['org_h'] = torch.as_tensor(org_height, dtype=torch.int64)\n        annotation['org_w'] = torch.as_tensor(org_width, dtype=torch.int64)\n\n        return image, annotation","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:41.791071Z","iopub.execute_input":"2023-10-12T20:33:41.791867Z","iopub.status.idle":"2023-10-12T20:33:41.804046Z","shell.execute_reply.started":"2023-10-12T20:33:41.791832Z","shell.execute_reply":"2023-10-12T20:33:41.803004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading Data","metadata":{}},{"cell_type":"code","source":"# Images\ntrain_data_dir = '/kaggle/input/sds-dataset/compressed/images/train'\ntest_data_dir = '/kaggle/input/sds-dataset/compressed/images/val'\n# Annotations\ntrain_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_train.json'\ntest_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_val.json'","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:41.806330Z","iopub.execute_input":"2023-10-12T20:33:41.807131Z","iopub.status.idle":"2023-10-12T20:33:41.821871Z","shell.execute_reply.started":"2023-10-12T20:33:41.807100Z","shell.execute_reply":"2023-10-12T20:33:41.820865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if Cuda is available\nprint(f'Cuda available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    # If yes, use GPU\n    device = torch.device('cuda')\nelse:\n    # If no, use CPU\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:41.823066Z","iopub.execute_input":"2023-10-12T20:33:41.823396Z","iopub.status.idle":"2023-10-12T20:33:41.857009Z","shell.execute_reply.started":"2023-10-12T20:33:41.823366Z","shell.execute_reply":"2023-10-12T20:33:41.856385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize = (256, 256)\nbatch_size = 32","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:41.858125Z","iopub.execute_input":"2023-10-12T20:33:41.858622Z","iopub.status.idle":"2023-10-12T20:33:41.875024Z","shell.execute_reply.started":"2023-10-12T20:33:41.858586Z","shell.execute_reply":"2023-10-12T20:33:41.873941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Datasets\ntrain_dataset = SDSDataset(train_data_dir, train_annotation_dir, resize)\ntest_dataset = SDSDataset(test_data_dir, test_annotation_dir, resize)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:41.876426Z","iopub.execute_input":"2023-10-12T20:33:41.876825Z","iopub.status.idle":"2023-10-12T20:33:42.376573Z","shell.execute_reply.started":"2023-10-12T20:33:41.876796Z","shell.execute_reply":"2023-10-12T20:33:42.375486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Dataloader\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader_train = DataLoader(train_dataset,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               collate_fn=collate_fn)\ndata_loader_test = DataLoader(test_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:42.378096Z","iopub.execute_input":"2023-10-12T20:33:42.378782Z","iopub.status.idle":"2023-10-12T20:33:42.499909Z","shell.execute_reply.started":"2023-10-12T20:33:42.378745Z","shell.execute_reply":"2023-10-12T20:33:42.498867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Image and Annotations","metadata":{}},{"cell_type":"code","source":"image, annotations = train_dataset[1000]\n# Convert the image tensor to NumPy and permute it for visualization\nimage_np = image.permute(1, 2, 0).numpy()\n\nfig, ax = plt.subplots(1)\nax.imshow(image_np)\n\n# Get bounding boxes and labels from annotations\nboxes = annotations['boxes'].numpy()\nlabels = annotations['labels'].numpy()\n\n# Loop through each bounding box and draw it\nfor i, box in enumerate(boxes):\n    x1, y1, x2, y2 = box\n    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n    ax.add_patch(rect)\n    plt.text(x1, y1, str(labels[i]), color='white')  # Optional: Add label text\n\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:42.501275Z","iopub.execute_input":"2023-10-12T20:33:42.502278Z","iopub.status.idle":"2023-10-12T20:33:42.928439Z","shell.execute_reply.started":"2023-10-12T20:33:42.502244Z","shell.execute_reply":"2023-10-12T20:33:42.927629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotations","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:42.931848Z","iopub.execute_input":"2023-10-12T20:33:42.932408Z","iopub.status.idle":"2023-10-12T20:33:42.969985Z","shell.execute_reply.started":"2023-10-12T20:33:42.932377Z","shell.execute_reply":"2023-10-12T20:33:42.969057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training model","metadata":{}},{"cell_type":"code","source":"modules = list(resnet18(weights=ResNet18_Weights.DEFAULT).children())[:-2]\nbackbone = nn.Sequential(*modules)\nbackbone.out_channels = 512","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:42.971051Z","iopub.execute_input":"2023-10-12T20:33:42.971786Z","iopub.status.idle":"2023-10-12T20:33:43.467463Z","shell.execute_reply.started":"2023-10-12T20:33:42.971755Z","shell.execute_reply":"2023-10-12T20:33:43.466564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Anchor Generator\nanchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:43.468641Z","iopub.execute_input":"2023-10-12T20:33:43.469582Z","iopub.status.idle":"2023-10-12T20:33:43.514010Z","shell.execute_reply.started":"2023-10-12T20:33:43.469527Z","shell.execute_reply":"2023-10-12T20:33:43.513107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize FasterRCNN with Backbone and AnchorGenerator\nmodel = FasterRCNN(backbone=backbone,\n                   rpn_anchor_generator=anchor_generator,\n                   num_classes=train_dataset.num_classes)\n# Send model to device\nmodel.to(device)\n\nprint('')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:43.515127Z","iopub.execute_input":"2023-10-12T20:33:43.516025Z","iopub.status.idle":"2023-10-12T20:33:47.051539Z","shell.execute_reply.started":"2023-10-12T20:33:43.515993Z","shell.execute_reply":"2023-10-12T20:33:47.050627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:47.052979Z","iopub.execute_input":"2023-10-12T20:33:47.053311Z","iopub.status.idle":"2023-10-12T20:33:47.060482Z","shell.execute_reply.started":"2023-10-12T20:33:47.053280Z","shell.execute_reply":"2023-10-12T20:33:47.059684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0001\nnum_epochs = 50","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:47.061740Z","iopub.execute_input":"2023-10-12T20:33:47.062719Z","iopub.status.idle":"2023-10-12T20:33:47.072751Z","shell.execute_reply.started":"2023-10-12T20:33:47.062689Z","shell.execute_reply":"2023-10-12T20:33:47.071887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define learning rate, optimizer and scheduler\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = Adam(params, lr=learning_rate)\nlr_scheduler = StepLR(optimizer, step_size=1, gamma=0.9)","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:47.073944Z","iopub.execute_input":"2023-10-12T20:33:47.074829Z","iopub.status.idle":"2023-10-12T20:33:47.084558Z","shell.execute_reply.started":"2023-10-12T20:33:47.074798Z","shell.execute_reply":"2023-10-12T20:33:47.083659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, annotation_dir):\n\n        self.annotation_dir = annotation_dir\n\n        self.coco = pyco.COCO(annotation_dir)\n        self.image_ids = list(self.coco.imgs.keys())\n        self.annotations = self.get_annotations()\n\n        self.predictions = {\n            \"images\": self.annotations[\"images\"].copy(),\n            \"categories\": self.annotations[\"categories\"].copy(),\n            \"annotations\": None\n        }\n\n    def get_annotations(self):\n        with open(self.annotation_dir, 'r') as f:\n            data = json.load(f)\n\n        for d in data['annotations']:\n            d['iscrowd'] = 0\n\n        return data\n\n    def get_predictions(self, preds):\n        with open(os.path.join('/kaggle/working', preds), 'r') as f:\n            data = json.load(f)\n\n        for new_id, d in enumerate(data, start=1):\n            d['id'] = new_id\n            d['iscrowd'] = 0\n            d['area'] = d['bbox'][2] * d['bbox'][3]\n\n        return data\n\n    def evaluate(self, pred_file, n_imgs=-1):\n\n        self.predictions[\"annotations\"] = self.get_predictions(pred_file)\n\n        coco_ds = pyco.COCO()\n        coco_ds.dataset = self.annotations\n        coco_ds.createIndex()\n\n        coco_dt = pyco.COCO()\n        coco_dt.dataset = self.predictions\n        coco_dt.createIndex()\n\n        imgIds = sorted(coco_ds.getImgIds())\n\n        if n_imgs > 0:\n            imgIds = np.random.choice(imgIds, n_imgs)\n\n        cocoEval = COCOeval(coco_ds, coco_dt, 'bbox')\n        cocoEval.params.imgIds = imgIds\n        cocoEval.params.useCats = True\n        cocoEval.params.iouType = \"bbox\"\n\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n\n        return cocoEval","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:47.085828Z","iopub.execute_input":"2023-10-12T20:33:47.086369Z","iopub.status.idle":"2023-10-12T20:33:47.097140Z","shell.execute_reply.started":"2023-10-12T20:33:47.086338Z","shell.execute_reply":"2023-10-12T20:33:47.096233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start Training Process\nevaluator = Evaluator(test_annotation_dir)\n\nfor epoch in range(num_epochs):\n    print()\n    print(f'############### Epoch {epoch} ########################################')\n    print()\n    \n    # Training\n    model.train()\n    epoch_loss = 0\n    for images, targets in data_loader_train:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        sum_loss = sum(loss for loss in loss_dict.values())\n        epoch_loss += sum_loss\n\n        sum_loss.backward()\n\n        optimizer.step()\n        optimizer.zero_grad()\n    epoch_loss /= batch_size\n\n    # Evaluation\n    model.eval()\n    with torch.no_grad():\n        for images, targets in data_loader_test:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            images_id = [i['image_id'] for i in targets]\n\n            pred_dict = model(images)\n\n            # Get image id and original size\n            img_id = targets[0]['image_id'].item()\n            org_width = targets[0]['org_w'].item()\n            org_height = targets[0]['org_h'].item()\n\n            # For every prediction:\n            pred_list = []\n            for box, label, score in zip(pred_dict[0]['boxes'],\n                                         pred_dict[0]['labels'],\n                                         pred_dict[0]['scores']):\n\n                # Create Dictionary with\n                pred_dict_coco = dict()\n                pred_dict_coco['image_id'] = img_id\n                # Predicted Label\n                pred_dict_coco['category_id'] = label.item()\n                # Confidence Score\n                pred_dict_coco['score'] = score.item()\n                # Predicted Bounding Box\n                xmin = box[0].item() * (org_width/resize[1])\n                ymin = box[1].item() * (org_height/resize[0])\n                width = (box[2].item() - box[0].item()) * (org_width/resize[1])\n                height = (box[3].item() - box[1].item()) * (org_height/resize[0])\n                pred_dict_coco['bbox'] = [xmin, ymin, width, height]\n                # And append Dictionary to List\n                pred_list.append(pred_dict_coco)\n\n            time_id = time.strftime('%Y_%m_%d-%H_%M')\n            with open(os.path.join('/kaggle/working', 'preds.json'.format(time_id)), 'w') as f:\n                json.dump(pred_list, f, ensure_ascii=False, indent=4)\n    if epoch == 0:\n        best_average_loss = epoch_loss\n\n    # If model improved, save weights\n    if best_average_loss >= epoch_loss:\n        best_average_loss = epoch_loss\n        torch.save(\n            model.state_dict(),\n            'model.pth'\n        )\n\n    lr_scheduler.step()\n\n    time_id = time.strftime('%Y_%m_%d-%H_%M')\n    print()\n    print(f'[{time_id}] '\n          f'Loss: {epoch_loss} - LR: {str(lr_scheduler.get_last_lr()[0])} ')\n    \n    # calculating average precision with COCO\n    print('Evaluation')\n    evaluator.evaluate('/kaggle/working/preds.json')\n\nprint()\nprint('Training stopped')","metadata":{"execution":{"iopub.status.busy":"2023-10-12T20:33:47.114917Z","iopub.execute_input":"2023-10-12T20:33:47.115464Z","iopub.status.idle":"2023-10-12T21:27:10.892460Z","shell.execute_reply.started":"2023-10-12T20:33:47.115429Z","shell.execute_reply":"2023-10-12T21:27:10.891019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image, annotations = images[0], targets[0]\n# pred = preds[0]\n# # Convert the image tensor to NumPy and permute it for visualization\n# image_np = image.permute(1, 2, 0).cpu().numpy()\n\n# fig, ax = plt.subplots(1)\n# ax.imshow(image_np)\n\n# # Get bounding boxes and labels from annotations\n# boxes = annotations['boxes'].cpu().numpy()\n# labels = annotations['labels'].cpu().numpy()\n\n# # Loop through each bounding box and draw it\n# for i, box in enumerate(boxes):\n#     x1, y1, x2, y2 = box\n#     rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='g', facecolor='none')\n#     ax.add_patch(rect)\n#     plt.text(x1, y1, str(labels[i]), color='white')  # Optional: Add label text\n    \n# # Get bounding boxes and labels from annotations\n# boxes_pred = pred['boxes'].cpu().numpy()\n# labels_pred = pred['labels'].cpu().numpy()\n\n# for i, box in enumerate(boxes_pred):\n#     x1, y1, x2, y2 = box\n#     rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n#     ax.add_patch(rect)\n#     plt.text(x1, y1, str(labels_pred[i]), color='white')  # Optional: Add label text\n\n# plt.axis('off')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-12T21:27:10.893510Z","iopub.status.idle":"2023-10-12T21:27:10.894541Z","shell.execute_reply.started":"2023-10-12T21:27:10.894310Z","shell.execute_reply":"2023-10-12T21:27:10.894333Z"},"trusted":true},"execution_count":null,"outputs":[]}]}