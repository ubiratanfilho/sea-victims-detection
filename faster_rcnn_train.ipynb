{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using CNNs for Sea Victims Detection","metadata":{"id":"DeVcuI9DXMD2"}},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"resize = (256, 256)\nbatch_size = 32\nlearning_rate = 0.00001\nnum_epochs = 50\nearly_stopping_tolerance = 5\nearly_stopping_counter = 0","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:22.736639Z","iopub.execute_input":"2023-09-25T20:20:22.737050Z","iopub.status.idle":"2023-09-25T20:20:22.753315Z","shell.execute_reply.started":"2023-09-25T20:20:22.737016Z","shell.execute_reply":"2023-09-25T20:20:22.752043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:22.756573Z","iopub.execute_input":"2023-09-25T20:20:22.757430Z","iopub.status.idle":"2023-09-25T20:20:38.254002Z","shell.execute_reply.started":"2023-09-25T20:20:22.757394Z","shell.execute_reply":"2023-09-25T20:20:38.252737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport os\nimport argparse\n\nimport pycocotools.coco as pyco\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport torch\nfrom torchvision.transforms import Compose, ToTensor, Resize\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchvision.models import resnet18, resnet50, resnet101,\\\n    ResNet101_Weights, ResNet18_Weights, ResNet50_Weights\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:38.257180Z","iopub.execute_input":"2023-09-25T20:20:38.258623Z","iopub.status.idle":"2023-09-25T20:20:42.063620Z","shell.execute_reply.started":"2023-09-25T20:20:38.258582Z","shell.execute_reply":"2023-09-25T20:20:42.062523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Class","metadata":{}},{"cell_type":"code","source":"class SDSDataset(Dataset):\n    def __init__(self, root, annotation_file, resize):\n        self.root = root\n        self.coco = pyco.COCO(annotation_file)\n        self.ids = list(self.coco.imgs.keys())\n        self.num_classes = len(self.coco.cats)\n        self.resize = resize\n        self.transform = Compose([\n            Resize(resize),\n            ToTensor()\n            ])\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        coco = self.coco\n\n        # Image ID of the input image\n        img_id = self.ids[index]\n        # Annotation IDs from coco\n        ann_ids = coco.getAnnIds(img_id)\n        # Load Annotation for the input image\n        coco_annotation = coco.loadAnns(ann_ids)\n        # Get path for the input image\n        path = coco.loadImgs(img_id)[0]['file_name']\n\n        # Open input image\n        org_image = Image.open(os.path.join(self.root, path))\n\n        # Get size of input image\n        org_height = org_image.height\n        org_width = org_image.width\n\n        # Apply transformation (resize) to input image\n        image = self.transform(org_image)\n\n        # Get number of objects in the input image\n        num_objects = len(coco_annotation)\n\n        # Get bounding boxes and category labels\n        # Coco format: bbox = [xmin, ymin, width, height]\n        # Pytorch format: bbox = [xmin, ymin, xmax, ymax]\n        boxes = []\n        labels = []\n        for i in range(num_objects):\n            # Convert and resize boxes\n            xmin = coco_annotation[i]['bbox'][0] / (org_width/self.resize[1])\n            ymin = coco_annotation[i]['bbox'][1] / (org_height/self.resize[0])\n            xmax = xmin + coco_annotation[i]['bbox'][2] / (org_width/self.resize[1])\n            ymax = ymin + coco_annotation[i]['bbox'][3] / (org_height/self.resize[0])\n            labels.append(coco_annotation[i]['category_id'])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        # Convert to tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        img_id = torch.tensor([img_id])\n\n        # Get (rectangular) size of bbox\n        areas = []\n        for i in range(num_objects):\n            areas.append(coco_annotation[i]['area'])\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n\n        # Get Iscrowd\n        iscrowd = torch.zeros((num_objects,), dtype=torch.int64)\n\n        # Create annotation dictionary\n        annotation = dict()\n        annotation['boxes'] = boxes\n        annotation['labels'] = labels\n        annotation['image_id'] = img_id\n        annotation['area'] = areas\n        annotation['iscrowd'] = iscrowd\n\n        # Save width and height of the original image to rescale bounding boxes later on\n        annotation['org_h'] = torch.as_tensor(org_height, dtype=torch.int64)\n        annotation['org_w'] = torch.as_tensor(org_width, dtype=torch.int64)\n\n        return image, annotation","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:42.067422Z","iopub.execute_input":"2023-09-25T20:20:42.068095Z","iopub.status.idle":"2023-09-25T20:20:42.088358Z","shell.execute_reply.started":"2023-09-25T20:20:42.068053Z","shell.execute_reply":"2023-09-25T20:20:42.087193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading Data","metadata":{}},{"cell_type":"code","source":"# Images\ntrain_data_dir = '/kaggle/input/sds-dataset/compressed/images/train'\ntest_data_dir = '/kaggle/input/sds-dataset/compressed/images/val'\n# Annotations\ntrain_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_train.json'\ntest_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_val.json'","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:42.090259Z","iopub.execute_input":"2023-09-25T20:20:42.092037Z","iopub.status.idle":"2023-09-25T20:20:42.101951Z","shell.execute_reply.started":"2023-09-25T20:20:42.091992Z","shell.execute_reply":"2023-09-25T20:20:42.100723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if Cuda is available\nprint(f'Cuda available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    # If yes, use GPU\n    device = torch.device('cuda')\nelse:\n    # If no, use CPU\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:42.103641Z","iopub.execute_input":"2023-09-25T20:20:42.104098Z","iopub.status.idle":"2023-09-25T20:20:42.154238Z","shell.execute_reply.started":"2023-09-25T20:20:42.104062Z","shell.execute_reply":"2023-09-25T20:20:42.152822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Datasets\ntrain_dataset = SDSDataset(train_data_dir, train_annotation_dir, resize)\ntest_dataset = SDSDataset(test_data_dir, test_annotation_dir, resize)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:42.156102Z","iopub.execute_input":"2023-09-25T20:20:42.156696Z","iopub.status.idle":"2023-09-25T20:20:42.831269Z","shell.execute_reply.started":"2023-09-25T20:20:42.156651Z","shell.execute_reply":"2023-09-25T20:20:42.830131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Dataloader\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader_train = DataLoader(train_dataset,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               collate_fn=collate_fn)\ndata_loader_test = DataLoader(test_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:42.833085Z","iopub.execute_input":"2023-09-25T20:20:42.833514Z","iopub.status.idle":"2023-09-25T20:20:42.990047Z","shell.execute_reply.started":"2023-09-25T20:20:42.833475Z","shell.execute_reply":"2023-09-25T20:20:42.988794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Image and Annotations","metadata":{}},{"cell_type":"code","source":"image, annotations = train_dataset[1000]\n# Convert the image tensor to NumPy and permute it for visualization\nimage_np = image.permute(1, 2, 0).numpy()\n\nfig, ax = plt.subplots(1)\nax.imshow(image_np)\n\n# Get bounding boxes and labels from annotations\nboxes = annotations['boxes'].numpy()\nlabels = annotations['labels'].numpy()\n\n# Loop through each bounding box and draw it\nfor i, box in enumerate(boxes):\n    x1, y1, x2, y2 = box\n    rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')\n    ax.add_patch(rect)\n    plt.text(x1, y1, str(labels[i]), color='white')  # Optional: Add label text\n\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:42.991586Z","iopub.execute_input":"2023-09-25T20:20:42.992044Z","iopub.status.idle":"2023-09-25T20:20:43.507131Z","shell.execute_reply.started":"2023-09-25T20:20:42.992005Z","shell.execute_reply":"2023-09-25T20:20:43.505739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training model","metadata":{}},{"cell_type":"code","source":"modules = list(resnet18(weights=ResNet18_Weights.DEFAULT).children())[:-2]\nbackbone = nn.Sequential(*modules)\nbackbone.out_channels = 512","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:43.510618Z","iopub.execute_input":"2023-09-25T20:20:43.510947Z","iopub.status.idle":"2023-09-25T20:20:44.168592Z","shell.execute_reply.started":"2023-09-25T20:20:43.510919Z","shell.execute_reply":"2023-09-25T20:20:44.167512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Anchor Generator\nanchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:44.170249Z","iopub.execute_input":"2023-09-25T20:20:44.170699Z","iopub.status.idle":"2023-09-25T20:20:44.228217Z","shell.execute_reply.started":"2023-09-25T20:20:44.170657Z","shell.execute_reply":"2023-09-25T20:20:44.226802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize FasterRCNN with Backbone and AnchorGenerator\nmodel = FasterRCNN(backbone=backbone,\n                   rpn_anchor_generator=anchor_generator,\n                   num_classes=train_dataset.num_classes)\n# Send model to device\nmodel.to(device)\n\nprint('')","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:44.229818Z","iopub.execute_input":"2023-09-25T20:20:44.230869Z","iopub.status.idle":"2023-09-25T20:20:47.858819Z","shell.execute_reply.started":"2023-09-25T20:20:44.230817Z","shell.execute_reply":"2023-09-25T20:20:47.857492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define learning rate, optimizer and scheduler\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = Adam(params, lr=learning_rate)\nlr_scheduler = StepLR(optimizer, step_size=1, gamma=0.9)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:47.860698Z","iopub.execute_input":"2023-09-25T20:20:47.861955Z","iopub.status.idle":"2023-09-25T20:20:47.869152Z","shell.execute_reply.started":"2023-09-25T20:20:47.861916Z","shell.execute_reply":"2023-09-25T20:20:47.868075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Start Training Process\nmodel.train()\nfor epoch in range(num_epochs):\n    # Training\n    for images, targets in data_loader_train:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        sum_loss = sum(loss for loss in loss_dict.values())\n\n        sum_loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    # Evaluation\n    with torch.no_grad():\n        for images, targets in data_loader_test:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            loss_dict = model(images, targets)\n            average_loss = sum(loss for loss in loss_dict.values()) / batch_size\n\n    if epoch == 0:\n        best_average_loss = average_loss\n\n    # If model improved, save weights\n    if best_average_loss >= average_loss:\n        best_average_loss = average_loss\n        early_stopping_counter = 0\n        torch.save(\n            model.state_dict(),\n            'model.pth'\n        )\n\n    # Otherwise, reduce learning rate\n    else:\n        early_stopping_counter += 1\n        lr_scheduler.step()\n\n    time_id = time.strftime('%Y_%m_%d-%H_%M')\n    print(f'[{time_id}] '\n          f'Epoch {epoch} of {num_epochs} - Loss: {average_loss} - LR: {str(lr_scheduler.get_last_lr()[0])} '\n          f'- Early Stopping: {early_stopping_counter}/{early_stopping_tolerance}')\n\n    if early_stopping_tolerance == early_stopping_counter:\n        break\n\nprint('Training stopped')\n\n# if args.create_prediction_file:\n#     # Create prediction file in coco format:\n#     generate_prediction_file(model, data_loader_test, device, resize)","metadata":{"execution":{"iopub.status.busy":"2023-09-25T20:20:47.870913Z","iopub.execute_input":"2023-09-25T20:20:47.871602Z","iopub.status.idle":"2023-09-25T20:46:27.954458Z","shell.execute_reply.started":"2023-09-25T20:20:47.871544Z","shell.execute_reply":"2023-09-25T20:46:27.952709Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
