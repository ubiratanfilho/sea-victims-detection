{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sea Victims Detection - Inference\n\nhttps://www.kaggle.com/code/ubiratanfilho/sea-victims-detection-inference","metadata":{"id":"DeVcuI9DXMD2"}},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:46.169890Z","iopub.execute_input":"2023-10-07T18:59:46.170361Z","iopub.status.idle":"2023-10-07T18:59:54.996011Z","shell.execute_reply.started":"2023-10-07T18:59:46.170331Z","shell.execute_reply":"2023-10-07T18:59:54.994890Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (2.0.7)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.23.5)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.40.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nimport os\nimport argparse\nimport json\n\nimport pycocotools.coco as pyco\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport numpy as np\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport torch\nfrom torchvision.transforms import Compose, ToTensor, Resize\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchvision.models import resnet18, resnet50, resnet101,\\\n    ResNet101_Weights, ResNet18_Weights, ResNet50_Weights\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:54.998708Z","iopub.execute_input":"2023-10-07T18:59:54.999785Z","iopub.status.idle":"2023-10-07T18:59:55.006550Z","shell.execute_reply.started":"2023-10-07T18:59:54.999745Z","shell.execute_reply":"2023-10-07T18:59:55.005660Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Class","metadata":{}},{"cell_type":"code","source":"class SDSDataset(Dataset):\n    def __init__(self, root, annotation_file, resize):\n        self.root = root\n        self.coco = pyco.COCO(annotation_file)\n        self.ids = list(self.coco.imgs.keys())\n        self.num_classes = len(self.coco.cats)\n        self.resize = resize\n        self.transform = Compose([\n            Resize(resize),\n            ToTensor()\n            ])\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        coco = self.coco\n\n        # Image ID of the input image\n        img_id = self.ids[index]\n        # Annotation IDs from coco\n        ann_ids = coco.getAnnIds(img_id)\n        # Load Annotation for the input image\n        coco_annotation = coco.loadAnns(ann_ids)\n        # Get path for the input image\n        path = coco.loadImgs(img_id)[0]['file_name']\n\n        # Open input image\n        org_image = Image.open(os.path.join(self.root, path))\n\n        # Get size of input image\n        org_height = org_image.height\n        org_width = org_image.width\n\n        # Apply transformation (resize) to input image\n        image = self.transform(org_image)\n\n        # Get number of objects in the input image\n        num_objects = len(coco_annotation)\n\n        # Get bounding boxes and category labels\n        # Coco format: bbox = [xmin, ymin, width, height]\n        # Pytorch format: bbox = [xmin, ymin, xmax, ymax]\n        boxes = []\n        labels = []\n        for i in range(num_objects):\n            # Convert and resize boxes\n            xmin = coco_annotation[i]['bbox'][0] / (org_width/self.resize[1])\n            ymin = coco_annotation[i]['bbox'][1] / (org_height/self.resize[0])\n            xmax = xmin + coco_annotation[i]['bbox'][2] / (org_width/self.resize[1])\n            ymax = ymin + coco_annotation[i]['bbox'][3] / (org_height/self.resize[0])\n            labels.append(coco_annotation[i]['category_id'])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        # Convert to tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        img_id = torch.tensor([img_id])\n\n        # Get (rectangular) size of bbox\n        areas = []\n        for i in range(num_objects):\n            areas.append(coco_annotation[i]['area'])\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n\n        # Get Iscrowd\n        iscrowd = torch.zeros((num_objects,), dtype=torch.int64)\n\n        # Create annotation dictionary\n        annotation = dict()\n        annotation['boxes'] = boxes\n        annotation['labels'] = labels\n        annotation['image_id'] = img_id\n        annotation['area'] = areas\n        annotation['iscrowd'] = iscrowd\n\n        # Save width and height of the original image to rescale bounding boxes later on\n        annotation['org_h'] = torch.as_tensor(org_height, dtype=torch.int64)\n        annotation['org_w'] = torch.as_tensor(org_width, dtype=torch.int64)\n\n        return image, annotation","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.008163Z","iopub.execute_input":"2023-10-07T18:59:55.008497Z","iopub.status.idle":"2023-10-07T18:59:55.023015Z","shell.execute_reply.started":"2023-10-07T18:59:55.008464Z","shell.execute_reply":"2023-10-07T18:59:55.022051Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Reading Data","metadata":{}},{"cell_type":"code","source":"# Images\ntest_data_dir = '/kaggle/input/sds-dataset/compressed/images/val'\n# Annotations\ntest_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_val.json'","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.024638Z","iopub.execute_input":"2023-10-07T18:59:55.025797Z","iopub.status.idle":"2023-10-07T18:59:55.035422Z","shell.execute_reply.started":"2023-10-07T18:59:55.025765Z","shell.execute_reply":"2023-10-07T18:59:55.034559Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Check if Cuda is available\nprint(f'Cuda available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    # If yes, use GPU\n    device = torch.device('cuda')\nelse:\n    # If no, use CPU\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.038832Z","iopub.execute_input":"2023-10-07T18:59:55.039316Z","iopub.status.idle":"2023-10-07T18:59:55.045912Z","shell.execute_reply.started":"2023-10-07T18:59:55.039284Z","shell.execute_reply":"2023-10-07T18:59:55.044783Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Cuda available: True\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create Datasets\nresize = (256, 256)\ntrain_dataset = SDSDataset(train_data_dir, train_annotation_dir, resize)\ntest_dataset = SDSDataset(test_data_dir, test_annotation_dir, resize)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.047115Z","iopub.execute_input":"2023-10-07T18:59:55.048078Z","iopub.status.idle":"2023-10-07T18:59:55.611871Z","shell.execute_reply.started":"2023-10-07T18:59:55.048048Z","shell.execute_reply":"2023-10-07T18:59:55.610121Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"loading annotations into memory...\nDone (t=0.45s)\ncreating index...\nindex created!\nloading annotations into memory...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m resize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SDSDataset(train_data_dir, train_annotation_dir, resize)\n\u001b[0;32m----> 4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSDSDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_annotation_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[42], line 4\u001b[0m, in \u001b[0;36mSDSDataset.__init__\u001b[0;34m(self, root, annotation_file, resize)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root, annotation_file, resize):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m root\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco \u001b[38;5;241m=\u001b[39m \u001b[43mpyco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco\u001b[38;5;241m.\u001b[39mimgs\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco\u001b[38;5;241m.\u001b[39mcats)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pycocotools/coco.py:81\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading annotations into memory...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation file format \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(dataset))\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/sds-dataset/compressed/annotations/instances_test.json'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/sds-dataset/compressed/annotations/instances_test.json'","output_type":"error"}]},{"cell_type":"code","source":"# Create Dataloader\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader_train = DataLoader(train_dataset,\n                               batch_size=32,\n                               shuffle=True,\n                               collate_fn=collate_fn)\ndata_loader_test = DataLoader(test_dataset,\n                              batch_size=32,\n                              shuffle=True,\n                              collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.612972Z","iopub.status.idle":"2023-10-07T18:59:55.614080Z","shell.execute_reply.started":"2023-10-07T18:59:55.613848Z","shell.execute_reply":"2023-10-07T18:59:55.613871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Model","metadata":{}},{"cell_type":"code","source":"modules = list(resnet18(weights=ResNet18_Weights.DEFAULT).children())[:-2]\nbackbone = nn.Sequential(*modules)\nbackbone.out_channels = 512","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.615338Z","iopub.status.idle":"2023-10-07T18:59:55.616273Z","shell.execute_reply.started":"2023-10-07T18:59:55.616032Z","shell.execute_reply":"2023-10-07T18:59:55.616054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Anchor Generator\nanchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.617559Z","iopub.status.idle":"2023-10-07T18:59:55.618730Z","shell.execute_reply.started":"2023-10-07T18:59:55.618457Z","shell.execute_reply":"2023-10-07T18:59:55.618497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize FasterRCNN with Backbone and AnchorGenerator\nmodel = FasterRCNN(backbone=backbone,\n                   rpn_anchor_generator=anchor_generator,\n                   num_classes=train_dataset.num_classes)\n\nmodel.load_state_dict(torch.load('/kaggle/input/model-pytorch/model_20231007.pth'))\n\n# Send model to device\nmodel.to(device)\n\nprint('Model Loaded')","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.619896Z","iopub.status.idle":"2023-10-07T18:59:55.620862Z","shell.execute_reply.started":"2023-10-07T18:59:55.620640Z","shell.execute_reply":"2023-10-07T18:59:55.620662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"markdown","source":"### Outputting some images","metadata":{}},{"cell_type":"code","source":"model.eval()\ncntr = 0\n\nwith torch.no_grad():\n    for images, targets in data_loader_test:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        pred_dict = model(images)\n        break","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.621973Z","iopub.status.idle":"2023-10-07T18:59:55.622985Z","shell.execute_reply.started":"2023-10-07T18:59:55.622766Z","shell.execute_reply":"2023-10-07T18:59:55.622787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    # Get image id and original size\n    img_id = targets[i]['image_id'].item()\n    org_width = targets[i]['org_w'].item()\n    org_height = targets[i]['org_h'].item()\n\n    # Open image with id\n    path = '/kaggle/input/sds-dataset/compressed/images/val/' + test_dataset.coco.loadImgs(img_id)[0]['file_name']\n    image = Image.open(os.path.join(test_data_dir, path))\n    image = image.resize((256, 256), Image.LANCZOS)\n\n    draw = ImageDraw.Draw(image)\n    # real bounding boxes\n    for box, label in zip(targets[i]['boxes'], targets[i]['labels']):\n                # Rescale bounding box coordinates\n                xmin = box[0].item()\n                ymin = box[1].item()\n                xmax = box[2].item()\n                ymax = box[3].item()\n\n                cat = test_dataset.coco.loadCats(label.item())[0]['name']\n\n                draw.rectangle(((xmin, ymin), (xmax, ymax)), width=3, outline='green')\n                draw.text((xmin - 10, ymin - 10),\n                          cat.capitalize(),\n                          fill='green', align='left')\n\n    # predictions\n    # Draw predicted bounding boxes, labels and scores\n    for box, label, score in zip(pred_dict[i]['boxes'],\n                                 pred_dict[i]['labels'],\n                                 pred_dict[i]['scores']):\n        if score.item() < float(0.5):\n            continue\n\n    # Rescale bounding box coordinates\n        xmin = box[0].item()\n        ymin = box[1].item()\n        xmax = box[2].item()\n        ymax = box[3].item()\n\n        cat = test_dataset.coco.loadCats(label.item())[0]['name']\n\n        draw.rectangle(((xmin, ymin), (xmax, ymax)), width=2, outline='red')\n        draw.text((xmin-10, ymin-10),\n                  cat.capitalize(),\n                  fill='red', align='right',\n                  stroke_width=0.2)\n\n    display(image.resize((512, 512), Image.LANCZOS))","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.624298Z","iopub.status.idle":"2023-10-07T18:59:55.624731Z","shell.execute_reply.started":"2023-10-07T18:59:55.624500Z","shell.execute_reply":"2023-10-07T18:59:55.624536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculating Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"def generate_prediction_file(model, data_loader, device, resize):\n    model.eval()\n\n    # Create List for Predictions\n    pred_list = list()\n\n    with torch.no_grad():\n        for images, targets in data_loader:\n\n            images = list(image.to(device) for image in images)\n            pred_dict = model(images)\n\n            # Get image id and original size\n            img_id = targets[0]['image_id'].item()\n            org_width = targets[0]['org_w'].item()\n            org_height = targets[0]['org_h'].item()\n\n            # For every prediction:\n            for box, label, score in zip(pred_dict[0]['boxes'],\n                                         pred_dict[0]['labels'],\n                                         pred_dict[0]['scores']):\n\n                # Create Dictionary with\n                pred_dict_coco = dict()\n                pred_dict_coco['image_id'] = img_id\n                # Predicted Label\n                pred_dict_coco['category_id'] = label.item()\n                # Confidence Score\n                pred_dict_coco['score'] = score.item()\n                # Predicted Bounding Box\n                xmin = box[0].item() * (org_width/resize[1])\n                ymin = box[1].item() * (org_height/resize[0])\n                width = (box[2].item() - box[0].item()) * (org_width/resize[1])\n                height = (box[3].item() - box[1].item()) * (org_height/resize[0])\n                pred_dict_coco['bbox'] = [xmin, ymin, width, height]\n                # And append Dictionary to List\n                pred_list.append(pred_dict_coco)\n\n    time_id = time.strftime('%Y_%m_%d-%H_%M')\n    with open(os.path.join('/kaggle/working', 'prediction{}.json'.format(time_id)), 'w') as f:\n        json.dump(pred_list, f, ensure_ascii=False, indent=4)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.626043Z","iopub.status.idle":"2023-10-07T18:59:55.627011Z","shell.execute_reply.started":"2023-10-07T18:59:55.626794Z","shell.execute_reply":"2023-10-07T18:59:55.626814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_prediction_file(model, data_loader_test, device, resize)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.628114Z","iopub.status.idle":"2023-10-07T18:59:55.629078Z","shell.execute_reply.started":"2023-10-07T18:59:55.628862Z","shell.execute_reply":"2023-10-07T18:59:55.628884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Evaluator:\n    def __init__(self, annotation_dir):\n\n        self.annotation_dir = annotation_dir\n\n        self.coco = COCO(annotation_dir)\n        self.image_ids = list(self.coco.imgs.keys())\n        self.annotations = self.get_annotations()\n\n        self.predictions = {\n            \"images\": self.annotations[\"images\"].copy(),\n            \"categories\": self.annotations[\"categories\"].copy(),\n            \"annotations\": None\n        }\n\n    def get_annotations(self):\n        with open(self.annotation_dir, 'r') as f:\n            data = json.load(f)\n\n        for d in data['annotations']:\n            d['iscrowd'] = 0\n\n        return data\n\n    def get_predictions(self, preds):\n        with open(os.path.join('/kaggle/working', preds), 'r') as f:\n            data = json.load(f)\n\n        for new_id, d in enumerate(data, start=1):\n            d['id'] = new_id\n            d['iscrowd'] = 0\n            d['area'] = d['bbox'][2] * d['bbox'][3]\n\n        return data\n\n    def evaluate(self, pred_file, n_imgs=-1):\n\n        self.predictions[\"annotations\"] = self.get_predictions(pred_file)\n\n        coco_ds = COCO()\n        coco_ds.dataset = self.annotations\n        coco_ds.createIndex()\n\n        coco_dt = COCO()\n        coco_dt.dataset = self.predictions\n        coco_dt.createIndex()\n\n        imgIds = sorted(coco_ds.getImgIds())\n\n        if n_imgs > 0:\n            imgIds = np.random.choice(imgIds, n_imgs)\n\n        cocoEval = COCOeval(coco_ds, coco_dt, 'bbox')\n        cocoEval.params.imgIds = imgIds\n        cocoEval.params.useCats = True\n        cocoEval.params.iouType = \"bbox\"\n\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n\n        return cocoEval","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.630207Z","iopub.status.idle":"2023-10-07T18:59:55.631174Z","shell.execute_reply.started":"2023-10-07T18:59:55.630927Z","shell.execute_reply":"2023-10-07T18:59:55.630949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluator = Evaluator(test_annotation_dir)\n\nevaluator.evaluate('/kaggle/working/prediction2023_10_07-18_50.json')","metadata":{"execution":{"iopub.status.busy":"2023-10-07T18:59:55.632267Z","iopub.status.idle":"2023-10-07T18:59:55.634280Z","shell.execute_reply.started":"2023-10-07T18:59:55.634041Z","shell.execute_reply":"2023-10-07T18:59:55.634063Z"},"trusted":true},"execution_count":null,"outputs":[]}]}