{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sea Victims Detection - Inference\n\nhttps://www.kaggle.com/code/ubiratanfilho/sea-victims-detection-inference","metadata":{"id":"DeVcuI9DXMD2"}},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"resize = (256, 256)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:51:45.041435Z","iopub.execute_input":"2023-10-01T19:51:45.041803Z","iopub.status.idle":"2023-10-01T19:51:45.046242Z","shell.execute_reply.started":"2023-10-01T19:51:45.041774Z","shell.execute_reply":"2023-10-01T19:51:45.045312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:51:15.259435Z","iopub.execute_input":"2023-10-01T19:51:15.259799Z","iopub.status.idle":"2023-10-01T19:51:24.768993Z","shell.execute_reply.started":"2023-10-01T19:51:15.259769Z","shell.execute_reply":"2023-10-01T19:51:24.767950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport os\nimport argparse\n\nimport pycocotools.coco as pyco\n\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimport torch\nfrom torchvision.transforms import Compose, ToTensor, Resize\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchvision.models import resnet18, resnet50, resnet101,\\\n    ResNet101_Weights, ResNet18_Weights, ResNet50_Weights\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:08:42.135778Z","iopub.execute_input":"2023-10-01T20:08:42.136101Z","iopub.status.idle":"2023-10-01T20:08:42.143362Z","shell.execute_reply.started":"2023-10-01T20:08:42.136070Z","shell.execute_reply":"2023-10-01T20:08:42.142116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Class","metadata":{}},{"cell_type":"code","source":"class SDSDataset(Dataset):\n    def __init__(self, root, annotation_file, resize):\n        self.root = root\n        self.coco = pyco.COCO(annotation_file)\n        self.ids = list(self.coco.imgs.keys())\n        self.num_classes = len(self.coco.cats)\n        self.resize = resize\n        self.transform = Compose([\n            Resize(resize),\n            ToTensor()\n            ])\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        coco = self.coco\n\n        # Image ID of the input image\n        img_id = self.ids[index]\n        # Annotation IDs from coco\n        ann_ids = coco.getAnnIds(img_id)\n        # Load Annotation for the input image\n        coco_annotation = coco.loadAnns(ann_ids)\n        # Get path for the input image\n        path = coco.loadImgs(img_id)[0]['file_name']\n\n        # Open input image\n        org_image = Image.open(os.path.join(self.root, path))\n\n        # Get size of input image\n        org_height = org_image.height\n        org_width = org_image.width\n\n        # Apply transformation (resize) to input image\n        image = self.transform(org_image)\n\n        # Get number of objects in the input image\n        num_objects = len(coco_annotation)\n\n        # Get bounding boxes and category labels\n        # Coco format: bbox = [xmin, ymin, width, height]\n        # Pytorch format: bbox = [xmin, ymin, xmax, ymax]\n        boxes = []\n        labels = []\n        for i in range(num_objects):\n            # Convert and resize boxes\n            xmin = coco_annotation[i]['bbox'][0] / (org_width/self.resize[1])\n            ymin = coco_annotation[i]['bbox'][1] / (org_height/self.resize[0])\n            xmax = xmin + coco_annotation[i]['bbox'][2] / (org_width/self.resize[1])\n            ymax = ymin + coco_annotation[i]['bbox'][3] / (org_height/self.resize[0])\n            labels.append(coco_annotation[i]['category_id'])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        # Convert to tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        img_id = torch.tensor([img_id])\n\n        # Get (rectangular) size of bbox\n        areas = []\n        for i in range(num_objects):\n            areas.append(coco_annotation[i]['area'])\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n\n        # Get Iscrowd\n        iscrowd = torch.zeros((num_objects,), dtype=torch.int64)\n\n        # Create annotation dictionary\n        annotation = dict()\n        annotation['boxes'] = boxes\n        annotation['labels'] = labels\n        annotation['image_id'] = img_id\n        annotation['area'] = areas\n        annotation['iscrowd'] = iscrowd\n\n        # Save width and height of the original image to rescale bounding boxes later on\n        annotation['org_h'] = torch.as_tensor(org_height, dtype=torch.int64)\n        annotation['org_w'] = torch.as_tensor(org_width, dtype=torch.int64)\n\n        return image, annotation","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:51:28.218728Z","iopub.execute_input":"2023-10-01T19:51:28.219522Z","iopub.status.idle":"2023-10-01T19:51:28.235191Z","shell.execute_reply.started":"2023-10-01T19:51:28.219488Z","shell.execute_reply":"2023-10-01T19:51:28.234088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading Data","metadata":{}},{"cell_type":"code","source":"# Images\ntrain_data_dir = '/kaggle/input/sds-dataset/compressed/images/train'\ntest_data_dir = '/kaggle/input/sds-dataset/compressed/images/val'\n# Annotations\ntrain_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_train.json'\ntest_annotation_dir = '/kaggle/input/sds-dataset/compressed/annotations/instances_val.json'","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:51:28.237267Z","iopub.execute_input":"2023-10-01T19:51:28.238729Z","iopub.status.idle":"2023-10-01T19:51:28.250288Z","shell.execute_reply.started":"2023-10-01T19:51:28.238699Z","shell.execute_reply":"2023-10-01T19:51:28.249414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if Cuda is available\nprint(f'Cuda available: {torch.cuda.is_available()}')\nif torch.cuda.is_available():\n    # If yes, use GPU\n    device = torch.device('cuda')\nelse:\n    # If no, use CPU\n    device = torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:51:28.251431Z","iopub.execute_input":"2023-10-01T19:51:28.252009Z","iopub.status.idle":"2023-10-01T19:51:28.288222Z","shell.execute_reply.started":"2023-10-01T19:51:28.251971Z","shell.execute_reply":"2023-10-01T19:51:28.287473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Datasets\ntrain_dataset = SDSDataset(train_data_dir, train_annotation_dir, resize)\ntest_dataset = SDSDataset(test_data_dir, test_annotation_dir, resize)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:51:52.506911Z","iopub.execute_input":"2023-10-01T19:51:52.507223Z","iopub.status.idle":"2023-10-01T19:51:53.014149Z","shell.execute_reply.started":"2023-10-01T19:51:52.507198Z","shell.execute_reply":"2023-10-01T19:51:53.013192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Dataloader\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader_train = DataLoader(train_dataset,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               collate_fn=collate_fn)\ndata_loader_test = DataLoader(test_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:51:55.951486Z","iopub.execute_input":"2023-10-01T19:51:55.951901Z","iopub.status.idle":"2023-10-01T19:51:55.958161Z","shell.execute_reply.started":"2023-10-01T19:51:55.951871Z","shell.execute_reply":"2023-10-01T19:51:55.957240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading Model","metadata":{}},{"cell_type":"code","source":"modules = list(resnet18(weights=ResNet18_Weights.DEFAULT).children())[:-2]\nbackbone = nn.Sequential(*modules)\nbackbone.out_channels = 512","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:57:27.986209Z","iopub.execute_input":"2023-10-01T19:57:27.986571Z","iopub.status.idle":"2023-10-01T19:57:28.212607Z","shell.execute_reply.started":"2023-10-01T19:57:27.986542Z","shell.execute_reply":"2023-10-01T19:57:28.211658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Anchor Generator\nanchor_generator = AnchorGenerator(sizes=((8, 16, 32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:57:28.754857Z","iopub.execute_input":"2023-10-01T19:57:28.755506Z","iopub.status.idle":"2023-10-01T19:57:28.760864Z","shell.execute_reply.started":"2023-10-01T19:57:28.755458Z","shell.execute_reply":"2023-10-01T19:57:28.759894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize FasterRCNN with Backbone and AnchorGenerator\nmodel = FasterRCNN(backbone=backbone,\n                   rpn_anchor_generator=anchor_generator,\n                   num_classes=train_dataset.num_classes)\n\nmodel.load_state_dict(torch.load('/kaggle/input/model-pytorch/model.pth'))\n\n# Send model to device\nmodel.to(device)\n\nprint('Model Loaded')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T19:57:29.592642Z","iopub.execute_input":"2023-10-01T19:57:29.592963Z","iopub.status.idle":"2023-10-01T19:57:30.107454Z","shell.execute_reply.started":"2023-10-01T19:57:29.592939Z","shell.execute_reply":"2023-10-01T19:57:30.106403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"model.eval()\ncntr = 0\n\nwith torch.no_grad():\n    for images, targets in data_loader_test:\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        pred_dict = model(images)\n        break","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:36:27.053125Z","iopub.execute_input":"2023-10-01T20:36:27.053465Z","iopub.status.idle":"2023-10-01T20:36:30.933256Z","shell.execute_reply.started":"2023-10-01T20:36:27.053419Z","shell.execute_reply":"2023-10-01T20:36:30.932287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    # Get image id and original size\n    img_id = targets[i]['image_id'].item()\n    org_width = targets[i]['org_w'].item()\n    org_height = targets[i]['org_h'].item()\n\n    # Open image with id\n    path = '/kaggle/input/sds-dataset/compressed/images/val/' + test_dataset.coco.loadImgs(img_id)[0]['file_name']\n    image = Image.open(os.path.join(test_data_dir, path))\n    image = image.resize((256, 256), Image.LANCZOS)\n\n    draw = ImageDraw.Draw(image)\n    # real bounding boxes\n    for box, label in zip(targets[i]['boxes'], targets[i]['labels']):\n                # Rescale bounding box coordinates\n                xmin = box[0].item()\n                ymin = box[1].item()\n                xmax = box[2].item()\n                ymax = box[3].item()\n\n                cat = test_dataset.coco.loadCats(label.item())[0]['name']\n\n                draw.rectangle(((xmin, ymin), (xmax, ymax)), width=3, outline='green')\n                draw.text((xmin - 10, ymin - 10),\n                          cat.capitalize(),\n                          fill='green', align='left')\n\n    # predictions\n    # Draw predicted bounding boxes, labels and scores\n    for box, label, score in zip(pred_dict[i]['boxes'],\n                                 pred_dict[i]['labels'],\n                                 pred_dict[i]['scores']):\n        if score.item() < float(0.5):\n            continue\n\n    # Rescale bounding box coordinates\n        xmin = box[0].item()\n        ymin = box[1].item()\n        xmax = box[2].item()\n        ymax = box[3].item()\n\n        cat = test_dataset.coco.loadCats(label.item())[0]['name']\n\n        draw.rectangle(((xmin, ymin), (xmax, ymax)), width=3, outline='red')\n        draw.text((xmin-10, ymin-10),\n                  cat.capitalize(),\n                  fill='red', align='right')\n\n    display(image.resize((512, 512), Image.LANCZOS))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:38:55.330056Z","iopub.execute_input":"2023-10-01T20:38:55.330591Z","iopub.status.idle":"2023-10-01T20:38:58.078772Z","shell.execute_reply.started":"2023-10-01T20:38:55.330552Z","shell.execute_reply":"2023-10-01T20:38:58.077987Z"},"trusted":true},"execution_count":null,"outputs":[]}]}